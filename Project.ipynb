{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gtx9x-UEXVe"
      },
      "source": [
        "# Setup and Imports\n",
        "Importing required libraries for data analysis, visualization, machine learning, and statistical analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTC3NS6kEcsd",
        "outputId": "60afe8ff-8a6d-4916-96cf-e1bd8cac7155"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All packages imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Corrected Cyclone Machine Data Analysis Pipeline\n",
        "# Task 1: Cyclone Machine Data Analysis\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import median_abs_deviation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#import statsmodels.api as sm\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Set plotting parameters\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "print(\" All packages imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t18sbMnOEgjH"
      },
      "source": [
        "#  Configuration Settings\n",
        "Defining configuration parameters including file paths, sampling frequency, and analysis parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbVYX8iHEkvn",
        "outputId": "ba884045-43ea-454f-b723-fbe38335fd6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Configuration set and directories created\n",
            "Input file: data.xlsx\n",
            "Outputs: Task1/outputs\n",
            "Plots: Task1/plots\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    \"input_csv\": \"data.xlsx\",  # Support for Excel files\n",
        "    \"timestamp_col\": \"timestamp\",\n",
        "    \"freq_minutes\": 5,\n",
        "    \"vars\": [\n",
        "        \"Cyclone_Inlet_Gas_Temp\",\n",
        "        \"Cyclone_Gas_Outlet_Temp\",\n",
        "        \"Cyclone_Outlet_Gas_draft\",\n",
        "        \"Cyclone_cone_draft\",\n",
        "        \"Cyclone_Inlet_Draft\",\n",
        "        \"Cyclone_Material_Temp\",\n",
        "    ],\n",
        "    \"plots_dir\": \"Task1/plots\",\n",
        "    \"outputs_dir\": \"Task1/outputs\",\n",
        "    \"outliers\": {\"method\": \"mad\", \"k\": 5.0},\n",
        "    \"small_gap_limit_minutes\": 30,\n",
        "    \"shutdown_rules\": {\n",
        "        \"min_idle_minutes\": 30,\n",
        "        \"max_var_window_min\": 30,\n",
        "        \"near_zero_thresholds\": {\n",
        "            \"Cyclone_Outlet_Gas_draft\": 0.02,\n",
        "            \"Cyclone_cone_draft\": 0.02,\n",
        "        },\n",
        "        \"flat_std_thresholds\": {\n",
        "            \"Cyclone_Inlet_Gas_Temp\": 1.0,\n",
        "            \"Cyclone_Gas_Outlet_Temp\": 1.0,\n",
        "        }\n",
        "    },\n",
        "    \"cluster\": {\n",
        "        \"max_k\": 6,\n",
        "        \"min_k\": 3,\n",
        "        \"dbscan\": {\"eps\": 0.8, \"min_samples\": 200}\n",
        "    },\n",
        "    \"anomaly\": {\n",
        "        \"contamination\": 0.02,\n",
        "        \"merge_gap_minutes\": 10,\n",
        "        \"min_event_minutes\": 10\n",
        "    },\n",
        "    \"forecast\": {\n",
        "        \"target\": \"Cyclone_Inlet_Gas_Temp\",\n",
        "        \"horizon_steps\": 12,\n",
        "        \"train_ratio\": 0.8,\n",
        "        \"model\": \"ridge\"\n",
        "    },\n",
        "    \"random_state\": 42\n",
        "}\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(CONFIG[\"plots_dir\"], exist_ok=True)\n",
        "os.makedirs(CONFIG[\"outputs_dir\"], exist_ok=True)\n",
        "\n",
        "print(\"✓ Configuration set and directories created\")\n",
        "print(f\"Input file: {CONFIG['input_csv']}\")\n",
        "print(f\"Outputs: {CONFIG['outputs_dir']}\")\n",
        "print(f\"Plots: {CONFIG['plots_dir']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRTnsFRwEoVF"
      },
      "source": [
        "#  Helper Functions - Basic Utilities\n",
        "Utility functions for saving figures, creating directories, and basic data operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgNsJKfXEroM",
        "outputId": "57da70c5-672e-46fc-930c-6ee1880c9975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Basic helper functions defined\n"
          ]
        }
      ],
      "source": [
        "# Basic helper functions\n",
        "def savefig(path):\n",
        "    \"\"\"Save figure with consistent formatting\"\"\"\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "def auto_detect_timestamp_col(df):\n",
        "    \"\"\"Auto-detect timestamp column\"\"\"\n",
        "    possible_names = ['timestamp', 'time', 'datetime', 'date', 'Timestamp', 'Time', 'DateTime']\n",
        "    for col in df.columns:\n",
        "        if col in possible_names or 'time' in col.lower() or 'date' in col.lower():\n",
        "            return col\n",
        "    # Try parsing first few rows\n",
        "    for col in df.columns:\n",
        "        try:\n",
        "            pd.to_datetime(df[col].iloc[:5])\n",
        "            return col\n",
        "        except:\n",
        "            continue\n",
        "    return df.columns[0]\n",
        "\n",
        "def enforce_fixed_freq(df, ts_col, freq_minutes):\n",
        "    \"\"\"Enforce strict time frequency\"\"\"\n",
        "    df = df.copy()\n",
        "    df[ts_col] = pd.to_datetime(df[ts_col])\n",
        "    df = df.sort_values(ts_col).set_index(ts_col)\n",
        "    full_idx = pd.date_range(df.index.min(), df.index.max(), freq=f\"{freq_minutes}min\")\n",
        "    df = df.reindex(full_idx)\n",
        "    df.index.name = ts_col\n",
        "    return df\n",
        "\n",
        "def fill_small_gaps_ffill(df, limit_minutes, freq_minutes):\n",
        "    \"\"\"Forward fill small gaps\"\"\"\n",
        "    limit = int(limit_minutes / freq_minutes)\n",
        "    return df.ffill(limit=limit)\n",
        "\n",
        "def winsorize_mad(series, k):\n",
        "    \"\"\"Robust outlier handling using MAD\"\"\"\n",
        "    numeric_series = pd.to_numeric(series, errors='coerce')\n",
        "    med = np.nanmedian(numeric_series)\n",
        "    mad = median_abs_deviation(numeric_series, nan_policy=\"omit\")\n",
        "    if np.isnan(mad) or mad == 0:\n",
        "        return numeric_series.clip(numeric_series.quantile(0.01), numeric_series.quantile(0.99))\n",
        "    low = med - k * mad\n",
        "    high = med + k * mad\n",
        "    return numeric_series.clip(lower=low, upper=high)\n",
        "\n",
        "print(\" Basic helper functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY7ZpeVtEu0V"
      },
      "source": [
        "#  Helper Functions - Feature Engineering\n",
        "Functions for creating rolling statistics, lag features, and time-based features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvuCyEfJEugM",
        "outputId": "9e4c2aa0-9b95-49d4-8f9d-43e2ca9122bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Feature engineering functions defined\n"
          ]
        }
      ],
      "source": [
        "# Feature engineering functions\n",
        "def add_rolling_features(df, cols, windows=[12, 72]):\n",
        "    \"\"\"Add rolling statistics and deltas\"\"\"\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        for w in windows:\n",
        "            df[f\"{c}_rollmean_{w}\"] = df[c].rolling(w, min_periods=max(3, w//3)).mean()\n",
        "            df[f\"{c}_rollstd_{w}\"] = df[c].rolling(w, min_periods=max(3, w//3)).std()\n",
        "        df[f\"{c}_delta_1\"] = df[c].diff(1)\n",
        "    return df\n",
        "\n",
        "def ensure_series(x, index):\n",
        "    \"\"\"Ensure x is a pandas Series with given index\"\"\"\n",
        "    if isinstance(x, pd.Series):\n",
        "        return x.reindex(index)\n",
        "    else:\n",
        "        return pd.Series(x, index=index)\n",
        "\n",
        "def build_cluster_features(df, cols):\n",
        "    \"\"\"Build comprehensive feature matrix for clustering\"\"\"\n",
        "    X = pd.DataFrame(index=df.index)\n",
        "    X = df[cols].copy()\n",
        "    X = add_rolling_features(X, cols, windows=[12, 72])\n",
        "\n",
        "    # Create lag features\n",
        "    for c in cols:\n",
        "        for lag in [1, 2, 12]:\n",
        "            X[f\"{c}_lag{lag}\"] = df[c].shift(lag)\n",
        "\n",
        "    X = X.dropna()\n",
        "    return X\n",
        "\n",
        "print(\"Feature engineering functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GS7PFAV1E0jA"
      },
      "source": [
        "#  Helper Functions - Event Detection\n",
        "Functions for detecting contiguous events, shutdown periods, and operational state changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sawy0yD7E0HY",
        "outputId": "5d77e809-94c8-4ac2-db0a-558d57f1df72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Event detection functions defined\n"
          ]
        }
      ],
      "source": [
        "# Event detection functions\n",
        "def contiguous_events(mask: pd.Series, min_len_steps: int, merge_gap_steps: int):\n",
        "    \"\"\"Extract contiguous events from boolean mask\"\"\"\n",
        "    if not isinstance(mask, pd.Series):\n",
        "        mask = pd.Series(mask, index=mask.index if hasattr(mask, 'index') else None)\n",
        "\n",
        "    idx = mask.index\n",
        "    events = []\n",
        "    in_evt = False\n",
        "    start = None\n",
        "    last_true = None\n",
        "\n",
        "    for t, v in zip(idx, mask.values):\n",
        "        if v:\n",
        "            if not in_evt:\n",
        "                in_evt = True\n",
        "                start = t\n",
        "            last_true = t\n",
        "        else:\n",
        "            if in_evt:\n",
        "                end = last_true\n",
        "                events.append((start, end))\n",
        "                in_evt = False\n",
        "\n",
        "    if in_evt:\n",
        "        events.append((start, last_true))\n",
        "\n",
        "    # Merge nearby events\n",
        "    merged = []\n",
        "    for s, e in events:\n",
        "        if not merged:\n",
        "            merged.append([s, e])\n",
        "        else:\n",
        "            prev_s, prev_e = merged[-1]\n",
        "            gap = (s - prev_e) / pd.Timedelta(minutes=CONFIG[\"freq_minutes\"])\n",
        "            if gap <= merge_gap_steps:\n",
        "                merged[-1][1] = e\n",
        "            else:\n",
        "                merged.append([s, e])\n",
        "\n",
        "    # Filter by minimum length\n",
        "    filtered = []\n",
        "    for s, e in merged:\n",
        "        dur_steps = int(((e - s) / pd.Timedelta(minutes=CONFIG[\"freq_minutes\"])) + 1)\n",
        "        if dur_steps >= min_len_steps:\n",
        "            filtered.append((s, e))\n",
        "    return filtered\n",
        "\n",
        "def detect_shutdown_mask(df, cfg):\n",
        "    \"\"\"Detect shutdown/idle periods using multiple criteria\"\"\"\n",
        "    freq = CONFIG[\"freq_minutes\"]\n",
        "    w = max(1, int(cfg[\"max_var_window_min\"] / freq))\n",
        "\n",
        "    # Near-zero draft conditions\n",
        "    nz_masks = []\n",
        "    for col, thr in cfg[\"near_zero_thresholds\"].items():\n",
        "        if col in df:\n",
        "            nz_masks.append((df[col].abs() <= thr).astype(bool))\n",
        "\n",
        "    if nz_masks:\n",
        "        nz_mask = pd.concat(nz_masks, axis=1).all(axis=1).astype(bool)\n",
        "    else:\n",
        "        nz_mask = pd.Series(False, index=df.index)\n",
        "\n",
        "    # Flat temperature conditions\n",
        "    flat_masks = []\n",
        "    for col, thr in cfg[\"flat_std_thresholds\"].items():\n",
        "        if col in df:\n",
        "            flat_masks.append((df[col].rolling(w, min_periods=max(1, w//2)).std() <= thr).astype(bool))\n",
        "\n",
        "    if flat_masks:\n",
        "        flat_mask = pd.concat(flat_masks, axis=1).any(axis=1).astype(bool)\n",
        "    else:\n",
        "        flat_mask = pd.Series(False, index=df.index)\n",
        "\n",
        "    # Combine conditions\n",
        "    idle = (nz_mask | flat_mask).astype(bool)\n",
        "    idle = idle.reindex(df.index).fillna(False)\n",
        "\n",
        "    # Filter by minimum duration\n",
        "    min_len = max(1, int(cfg[\"min_idle_minutes\"] / freq))\n",
        "    run_ids = (idle != idle.shift(1)).cumsum()\n",
        "    out = pd.Series(False, index=idle.index)\n",
        "    for rid, grp in idle.groupby(run_ids):\n",
        "        if grp.iloc[0] and grp.sum() >= min_len:\n",
        "            out.loc[grp.index] = True\n",
        "    return out\n",
        "\n",
        "print(\"Event detection functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H1LPQvVE6ty"
      },
      "source": [
        "#  Data Loading and Initial Processing\n",
        "Loading the dataset, performing initial validation, and data quality checks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
            "Loading data...\n",
            "\n",
            "Collecting openpyxl\n",
            "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting et-xmlfile (from openpyxl)\n",
            "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
            "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: et-xmlfile, openpyxl\n",
            "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
            "WARNING: Ignoring invalid distribution ~ensorflow-intel (C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages)\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.2\n",
            "[notice] To update, run: C:\\Users\\Admin\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw data shape: (377719, 7)\n",
            "Raw columns: ['time', 'Cyclone_Inlet_Gas_Temp', 'Cyclone_Material_Temp', 'Cyclone_Outlet_Gas_draft', 'Cyclone_cone_draft', 'Cyclone_Gas_Outlet_Temp', 'Cyclone_Inlet_Draft']\n",
            "Using timestamp column: time\n",
            "Available variables: ['Cyclone_Inlet_Gas_Temp', 'Cyclone_Gas_Outlet_Temp', 'Cyclone_Outlet_Gas_draft', 'Cyclone_cone_draft', 'Cyclone_Inlet_Draft', 'Cyclone_Material_Temp']\n",
            "\n",
            "Sample of raw data:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time</th>\n",
              "      <th>Cyclone_Inlet_Gas_Temp</th>\n",
              "      <th>Cyclone_Gas_Outlet_Temp</th>\n",
              "      <th>Cyclone_Outlet_Gas_draft</th>\n",
              "      <th>Cyclone_cone_draft</th>\n",
              "      <th>Cyclone_Inlet_Draft</th>\n",
              "      <th>Cyclone_Material_Temp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2017-01-01 00:00:00</td>\n",
              "      <td>867.63</td>\n",
              "      <td>852.13</td>\n",
              "      <td>-189.54</td>\n",
              "      <td>-186.04</td>\n",
              "      <td>-145.9</td>\n",
              "      <td>910.42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2017-01-01 00:05:00</td>\n",
              "      <td>879.23</td>\n",
              "      <td>862.53</td>\n",
              "      <td>-184.33</td>\n",
              "      <td>-182.1</td>\n",
              "      <td>-149.76</td>\n",
              "      <td>918.14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2017-01-01 00:10:00</td>\n",
              "      <td>875.67</td>\n",
              "      <td>866.06</td>\n",
              "      <td>-181.26</td>\n",
              "      <td>-166.47</td>\n",
              "      <td>-145.01</td>\n",
              "      <td>924.18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2017-01-01 00:15:00</td>\n",
              "      <td>875.28</td>\n",
              "      <td>865.85</td>\n",
              "      <td>-179.15</td>\n",
              "      <td>-174.83</td>\n",
              "      <td>-142.82</td>\n",
              "      <td>923.15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2017-01-01 00:20:00</td>\n",
              "      <td>891.66</td>\n",
              "      <td>876.06</td>\n",
              "      <td>-178.32</td>\n",
              "      <td>-173.72</td>\n",
              "      <td>-143.39</td>\n",
              "      <td>934.26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 time Cyclone_Inlet_Gas_Temp Cyclone_Gas_Outlet_Temp  \\\n",
              "0 2017-01-01 00:00:00                 867.63                  852.13   \n",
              "1 2017-01-01 00:05:00                 879.23                  862.53   \n",
              "2 2017-01-01 00:10:00                 875.67                  866.06   \n",
              "3 2017-01-01 00:15:00                 875.28                  865.85   \n",
              "4 2017-01-01 00:20:00                 891.66                  876.06   \n",
              "\n",
              "  Cyclone_Outlet_Gas_draft Cyclone_cone_draft Cyclone_Inlet_Draft  \\\n",
              "0                  -189.54            -186.04              -145.9   \n",
              "1                  -184.33             -182.1             -149.76   \n",
              "2                  -181.26            -166.47             -145.01   \n",
              "3                  -179.15            -174.83             -142.82   \n",
              "4                  -178.32            -173.72             -143.39   \n",
              "\n",
              "  Cyclone_Material_Temp  \n",
              "0                910.42  \n",
              "1                918.14  \n",
              "2                924.18  \n",
              "3                923.15  \n",
              "4                934.26  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install openpyxl\n",
        "\n",
        "# Data loading and initial processing\n",
        "print(\"Loading data...\")\n",
        "np.random.seed(CONFIG[\"random_state\"])\n",
        "\n",
        "# Load data (support both CSV and Excel)\n",
        "file_ext = os.path.splitext(CONFIG[\"input_csv\"])[1].lower()\n",
        "if file_ext == \".csv\":\n",
        "    df_raw = pd.read_csv(CONFIG[\"input_csv\"])\n",
        "elif file_ext in [\".xls\", \".xlsx\"]:\n",
        "    df_raw = pd.read_excel(CONFIG[\"input_csv\"])\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported file format: {file_ext}\")\n",
        "\n",
        "print(f\"Raw data shape: {df_raw.shape}\")\n",
        "print(f\"Raw columns: {list(df_raw.columns)}\")\n",
        "\n",
        "# Auto-detect timestamp column\n",
        "ts_col = auto_detect_timestamp_col(df_raw)\n",
        "print(f\"Using timestamp column: {ts_col}\")\n",
        "\n",
        "# Check for available variables\n",
        "available_vars = [v for v in CONFIG[\"vars\"] if v in df_raw.columns]\n",
        "if not available_vars:\n",
        "    print(\"Warning: Expected variable names not found. Using all numeric columns.\")\n",
        "    available_vars = df_raw.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "print(f\"Available variables: {available_vars}\")\n",
        "df = df_raw[[ts_col] + available_vars].copy()\n",
        "\n",
        "print(\"\\nSample of raw data:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvTeOyBgFZsR"
      },
      "source": [
        "#  Time Series Preprocessing\n",
        "Enforcing fixed 5-minute intervals, handling missing timestamps, and data interpolation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4fx2SqHFEmz",
        "outputId": "91eef737-a5cc-4d62-b834-4f6f9634afcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enforcing fixed 5-minute frequency...\n",
            "After reindexing: (378580, 6)\n",
            "Missing values per column:\n",
            "Cyclone_Inlet_Gas_Temp      861\n",
            "Cyclone_Gas_Outlet_Temp     861\n",
            "Cyclone_Outlet_Gas_draft    861\n",
            "Cyclone_cone_draft          861\n",
            "Cyclone_Inlet_Draft         861\n",
            "Cyclone_Material_Temp       861\n",
            "dtype: int64\n",
            "\n",
            "Filling small gaps with forward fill...\n",
            "After gap filling - Missing values:\n",
            "Cyclone_Inlet_Gas_Temp      843\n",
            "Cyclone_Gas_Outlet_Temp     843\n",
            "Cyclone_Outlet_Gas_draft    843\n",
            "Cyclone_cone_draft          843\n",
            "Cyclone_Inlet_Draft         843\n",
            "Cyclone_Material_Temp       843\n",
            "dtype: int64\n",
            "\n",
            "Winsorizing outliers using MAD method...\n",
            "Cyclone_Inlet_Gas_Temp: [0.00, 1157.63] → [777.18, 987.58]\n",
            "Cyclone_Gas_Outlet_Temp: [13.79, 1375.00] → [700.97, 1042.07]\n",
            "Cyclone_Outlet_Gas_draft: [-456.66, 40.27] → [-395.36, -35.16]\n",
            "Cyclone_cone_draft: [-459.31, 488.86] → [-365.01, -32.11]\n",
            "Cyclone_Inlet_Draft: [-396.37, 41.64] → [-301.56, -37.36]\n",
            "Cyclone_Material_Temp: [-185.00, 1375.00] → [738.81, 1087.91]\n",
            "✓ Data preprocessing complete\n"
          ]
        }
      ],
      "source": [
        "# Time series preprocessing\n",
        "print(\"Enforcing fixed 5-minute frequency...\")\n",
        "df = enforce_fixed_freq(df, ts_col, CONFIG[\"freq_minutes\"])\n",
        "\n",
        "print(f\"After reindexing: {df.shape}\")\n",
        "print(f\"Missing values per column:\")\n",
        "missing_counts = df[available_vars].isnull().sum()\n",
        "print(missing_counts)\n",
        "\n",
        "print(\"\\nFilling small gaps with forward fill...\")\n",
        "df[available_vars] = fill_small_gaps_ffill(\n",
        "    df[available_vars],\n",
        "    CONFIG[\"small_gap_limit_minutes\"],\n",
        "    CONFIG[\"freq_minutes\"]\n",
        ")\n",
        "\n",
        "print(\"After gap filling - Missing values:\")\n",
        "print(df[available_vars].isnull().sum())\n",
        "\n",
        "print(\"\\nWinsorizing outliers using MAD method...\")\n",
        "for v in available_vars:\n",
        "    # Ensure column is numeric, coercing errors to NaN\n",
        "    df[v] = pd.to_numeric(df[v], errors='coerce')\n",
        "    # Drop NaNs temporarily for range calculation if needed, or handle them\n",
        "    # For robustness, calculate range only on non-NaN values\n",
        "    valid_data = df[v].dropna()\n",
        "    if not valid_data.empty:\n",
        "        original_range = f\"[{valid_data.min():.2f}, {valid_data.max():.2f}]\"\n",
        "        df[v] = winsorize_mad(df[v], CONFIG[\"outliers\"][\"k\"])\n",
        "        new_range = f\"[{df[v].min():.2f}, {df[v].max():.2f}]\"\n",
        "        print(f\"{v}: {original_range} → {new_range}\")\n",
        "    else:\n",
        "        print(f\"{v}: No valid numeric data to winsorize.\")\n",
        "\n",
        "\n",
        "print(\"✓ Data preprocessing complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v45sElllGZ9Q"
      },
      "source": [
        "#  Exploratory Data Analysis\n",
        "Generating summary statistics, correlation analysis, and data distribution insights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "cPW1BeocFbtG",
        "outputId": "2d6fddb0-cc74-459c-c6a8-4624172fdcb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating summary statistics...\n",
            "Summary Statistics:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Cyclone_Inlet_Gas_Temp</th>\n",
              "      <td>376417.0</td>\n",
              "      <td>867.846982</td>\n",
              "      <td>52.617870</td>\n",
              "      <td>777.18</td>\n",
              "      <td>856.27</td>\n",
              "      <td>882.38</td>\n",
              "      <td>901.1100</td>\n",
              "      <td>987.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cyclone_Gas_Outlet_Temp</th>\n",
              "      <td>376416.0</td>\n",
              "      <td>841.173925</td>\n",
              "      <td>82.463520</td>\n",
              "      <td>700.97</td>\n",
              "      <td>801.97</td>\n",
              "      <td>871.52</td>\n",
              "      <td>899.3000</td>\n",
              "      <td>1042.07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cyclone_Outlet_Gas_draft</th>\n",
              "      <td>376416.0</td>\n",
              "      <td>-185.497946</td>\n",
              "      <td>85.372483</td>\n",
              "      <td>-395.36</td>\n",
              "      <td>-247.19</td>\n",
              "      <td>-215.26</td>\n",
              "      <td>-170.1375</td>\n",
              "      <td>-35.16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cyclone_cone_draft</th>\n",
              "      <td>376417.0</td>\n",
              "      <td>-170.719891</td>\n",
              "      <td>79.138405</td>\n",
              "      <td>-365.01</td>\n",
              "      <td>-226.77</td>\n",
              "      <td>-198.56</td>\n",
              "      <td>-143.6500</td>\n",
              "      <td>-32.11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cyclone_Inlet_Draft</th>\n",
              "      <td>376415.0</td>\n",
              "      <td>-149.015702</td>\n",
              "      <td>63.773916</td>\n",
              "      <td>-301.56</td>\n",
              "      <td>-193.51</td>\n",
              "      <td>-169.46</td>\n",
              "      <td>-136.3000</td>\n",
              "      <td>-37.36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Cyclone_Material_Temp</th>\n",
              "      <td>376146.0</td>\n",
              "      <td>887.541680</td>\n",
              "      <td>83.687185</td>\n",
              "      <td>738.81</td>\n",
              "      <td>867.66</td>\n",
              "      <td>913.36</td>\n",
              "      <td>943.6500</td>\n",
              "      <td>1087.91</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             count        mean        std     min     25%  \\\n",
              "Cyclone_Inlet_Gas_Temp    376417.0  867.846982  52.617870  777.18  856.27   \n",
              "Cyclone_Gas_Outlet_Temp   376416.0  841.173925  82.463520  700.97  801.97   \n",
              "Cyclone_Outlet_Gas_draft  376416.0 -185.497946  85.372483 -395.36 -247.19   \n",
              "Cyclone_cone_draft        376417.0 -170.719891  79.138405 -365.01 -226.77   \n",
              "Cyclone_Inlet_Draft       376415.0 -149.015702  63.773916 -301.56 -193.51   \n",
              "Cyclone_Material_Temp     376146.0  887.541680  83.687185  738.81  867.66   \n",
              "\n",
              "                             50%       75%      max  \n",
              "Cyclone_Inlet_Gas_Temp    882.38  901.1100   987.58  \n",
              "Cyclone_Gas_Outlet_Temp   871.52  899.3000  1042.07  \n",
              "Cyclone_Outlet_Gas_draft -215.26 -170.1375   -35.16  \n",
              "Cyclone_cone_draft       -198.56 -143.6500   -32.11  \n",
              "Cyclone_Inlet_Draft      -169.46 -136.3000   -37.36  \n",
              "Cyclone_Material_Temp     913.36  943.6500  1087.91  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Summary statistics saved\n"
          ]
        }
      ],
      "source": [
        "# Summary statistics and correlations\n",
        "print(\"Generating summary statistics...\")\n",
        "\n",
        "# Save summary stats\n",
        "desc = df[available_vars].describe().T\n",
        "desc.to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"summary_stats.csv\"))\n",
        "\n",
        "print(\"Summary Statistics:\")\n",
        "display(desc)\n",
        "\n",
        "# Correlation matrix\n",
        "corr = df[available_vars].corr()\n",
        "corr.to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"correlations.csv\"))\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(corr, annot=True, cmap='coolwarm', center=0, square=True)\n",
        "plt.title(\"Variable Correlation Matrix\")\n",
        "savefig(os.path.join(CONFIG[\"plots_dir\"], \"correlation_matrix.png\"))\n",
        "plt.show()\n",
        "\n",
        "print(\"✓ Summary statistics saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjdZ1ah-Gf-I"
      },
      "source": [
        "#  EDA Visualizations\n",
        "Creating time series plots, correlation heatmaps, and distribution visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTVtwdhpGjP7",
        "outputId": "22f404e2-b543-43bc-8d7b-aed3a09db24d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating EDA plots...\n",
            "✓ EDA visualizations complete\n"
          ]
        }
      ],
      "source": [
        "# Create EDA visualizations\n",
        "print(\"Creating EDA plots...\")\n",
        "\n",
        "t0 = df.index.min()\n",
        "week_end = t0 + pd.Timedelta(days=7)\n",
        "year_end = t0 + pd.Timedelta(days=365)\n",
        "\n",
        "# Week slice visualization\n",
        "fig, axes = plt.subplots(len(available_vars), 1, figsize=(15, 3*len(available_vars)))\n",
        "if len(available_vars) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, v in enumerate(available_vars):\n",
        "    week_data = df.loc[t0:week_end, v]\n",
        "    axes[i].plot(week_data.index, week_data.values, linewidth=0.8)\n",
        "    axes[i].set_title(f\"{v} (Week Slice)\")\n",
        "    axes[i].set_ylabel(v)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"One Week Data Overview\", fontsize=16)\n",
        "savefig(os.path.join(CONFIG[\"plots_dir\"], \"eda_week_slice.png\"))\n",
        "plt.show()\n",
        "\n",
        "# Year slice visualization\n",
        "fig, axes = plt.subplots(len(available_vars), 1, figsize=(15, 3*len(available_vars)))\n",
        "if len(available_vars) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, v in enumerate(available_vars):\n",
        "    year_data = df.loc[t0:year_end, v]\n",
        "    axes[i].plot(year_data.index, year_data.values, linewidth=0.5, alpha=0.8)\n",
        "    axes[i].set_title(f\"{v} (Year Slice)\")\n",
        "    axes[i].set_ylabel(v)\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle(\"One Year Data Overview\", fontsize=16)\n",
        "savefig(os.path.join(CONFIG[\"plots_dir\"], \"eda_year_slice.png\"))\n",
        "plt.show()\n",
        "\n",
        "print(\" EDA visualizations complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUvDlQv1Gm4C"
      },
      "source": [
        "#  Shutdown Detection\n",
        "Identifying and analyzing machine shutdown and idle periods using multi-variable criteria."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "KneR0rbsGmKD",
        "outputId": "3a26ed1e-a746-4927-8f50-82822ff54de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detecting shutdown/idle periods...\n",
            "✓ Found 268 shutdown events\n",
            "Total downtime: 430705 minutes (7178.4 hours)\n",
            "Average shutdown duration: 1607 minutes\n",
            "Machine availability: 77.2%\n",
            "\n",
            "Top 5 longest shutdowns:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>duration_min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>2020-03-22 21:20:00</td>\n",
              "      <td>2020-05-02 00:30:00</td>\n",
              "      <td>57795.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>217</th>\n",
              "      <td>2019-09-14 08:10:00</td>\n",
              "      <td>2019-10-18 12:20:00</td>\n",
              "      <td>49215.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>2019-11-29 12:20:00</td>\n",
              "      <td>2019-12-24 22:50:00</td>\n",
              "      <td>36635.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>163</th>\n",
              "      <td>2018-09-25 15:55:00</td>\n",
              "      <td>2018-10-18 10:30:00</td>\n",
              "      <td>32800.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>211</th>\n",
              "      <td>2019-06-03 03:25:00</td>\n",
              "      <td>2019-06-22 03:25:00</td>\n",
              "      <td>27365.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  start                 end  duration_min\n",
              "261 2020-03-22 21:20:00 2020-05-02 00:30:00       57795.0\n",
              "217 2019-09-14 08:10:00 2019-10-18 12:20:00       49215.0\n",
              "255 2019-11-29 12:20:00 2019-12-24 22:50:00       36635.0\n",
              "163 2018-09-25 15:55:00 2018-10-18 10:30:00       32800.0\n",
              "211 2019-06-03 03:25:00 2019-06-22 03:25:00       27365.0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shutdown periods saved\n"
          ]
        }
      ],
      "source": [
        "# Shutdown detection\n",
        "print(\"Detecting shutdown/idle periods...\")\n",
        "\n",
        "shutdown_mask = detect_shutdown_mask(df, CONFIG[\"shutdown_rules\"])\n",
        "min_steps = int(CONFIG[\"shutdown_rules\"][\"min_idle_minutes\"] / CONFIG[\"freq_minutes\"])\n",
        "shutdown_events = contiguous_events(shutdown_mask, min_steps, merge_gap_steps=0)\n",
        "\n",
        "# Create shutdown dataframe\n",
        "shut_df = pd.DataFrame(shutdown_events, columns=[\"start\", \"end\"]) if shutdown_events else pd.DataFrame(columns=[\"start\", \"end\"])\n",
        "\n",
        "if not shut_df.empty:\n",
        "    shut_df[\"duration_min\"] = ((shut_df[\"end\"] - shut_df[\"start\"]) / pd.Timedelta(minutes=CONFIG[\"freq_minutes\"]) + 1) * CONFIG[\"freq_minutes\"]\n",
        "    total_downtime = shut_df[\"duration_min\"].sum()\n",
        "\n",
        "    print(f\"✓ Found {len(shut_df)} shutdown events\")\n",
        "    print(f\"Total downtime: {total_downtime:.0f} minutes ({total_downtime/60:.1f} hours)\")\n",
        "    print(f\"Average shutdown duration: {shut_df['duration_min'].mean():.0f} minutes\")\n",
        "    print(f\"Machine availability: {((len(df) - shutdown_mask.sum()) / len(df) * 100):.1f}%\")\n",
        "\n",
        "    print(\"\\nTop 5 longest shutdowns:\")\n",
        "    display(shut_df.nlargest(5, 'duration_min'))\n",
        "else:\n",
        "    print(\"No shutdown events detected with current thresholds\")\n",
        "\n",
        "# Save shutdown periods\n",
        "shut_df.to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"shutdown_periods.csv\"), index=False)\n",
        "print(\"Shutdown periods saved\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzaaCuOYGxK-"
      },
      "source": [
        "#  Shutdown Visualization\n",
        "Visualizing shutdown patterns, durations, and temporal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KR64d1bVGlDu",
        "outputId": "bb38df7d-214a-4f76-ffbd-8d730b77e25b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating shutdown visualizations...\n",
            "✓ Shutdown visualizations complete\n"
          ]
        }
      ],
      "source": [
        "# Shutdown visualization\n",
        "print(\"Creating shutdown visualizations...\")\n",
        "\n",
        "# Year view with shutdown bands\n",
        "plt.figure(figsize=(15, 8))\n",
        "v = available_vars[0]\n",
        "year_data = df.loc[t0:year_end]\n",
        "plt.plot(year_data.index, year_data[v], label=v, lw=0.6, alpha=0.8)\n",
        "\n",
        "# Add shutdown bands\n",
        "shutdown_label_added = False\n",
        "for s, e in shutdown_events:\n",
        "    if s <= year_end:\n",
        "        label = \"Shutdown Periods\" if not shutdown_label_added else \"\"\n",
        "        plt.axvspan(s, min(e, year_end), color=\"red\", alpha=0.15, label=label)\n",
        "        shutdown_label_added = True\n",
        "\n",
        "plt.legend()\n",
        "plt.title(\"Full Year View with Shutdown Periods Highlighted\")\n",
        "plt.ylabel(v)\n",
        "plt.grid(True, alpha=0.3)\n",
        "savefig(os.path.join(CONFIG[\"plots_dir\"], \"year_with_shutdowns.png\"))\n",
        "plt.show()\n",
        "\n",
        "# Shutdown frequency analysis\n",
        "if not shut_df.empty:\n",
        "    shut_df['month'] = pd.to_datetime(shut_df['start']).dt.month\n",
        "    monthly_counts = shut_df['month'].value_counts().sort_index()\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    monthly_counts.plot(kind='bar', color='skyblue', alpha=0.8)\n",
        "    plt.title(\"Shutdown Events by Month\")\n",
        "    plt.xlabel(\"Month\")\n",
        "    plt.ylabel(\"Number of Events\")\n",
        "    plt.xticks(rotation=0)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    savefig(os.path.join(CONFIG[\"plots_dir\"], \"shutdowns_by_month.png\"))\n",
        "    plt.show()\n",
        "\n",
        "print(\"Shutdown visualizations complete\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJBGcSBoJ3nW"
      },
      "source": [
        "#  Clustering Functions\n",
        "Defining optimized K-means clustering with silhouette scoring and elbow method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSUHtQohJHqw",
        "outputId": "a35adc9b-8f05-4a14-fb56-9b911772a57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Clustering functions defined\n"
          ]
        }
      ],
      "source": [
        "# Clustering functions\n",
        "def choose_kmeans_k(X_array, min_k, max_k, random_state):\n",
        "    \"\"\"Choose optimal K using silhouette analysis\"\"\"\n",
        "    best_k, best_score = None, -1\n",
        "    scores = []\n",
        "\n",
        "    for k in range(min_k, max_k + 1):\n",
        "        km = KMeans(n_clusters=k, random_state=random_state, n_init=10)\n",
        "        labels = km.fit_predict(X_array)\n",
        "        sc = silhouette_score(X_array, labels)\n",
        "        scores.append(sc)\n",
        "\n",
        "        if sc > best_score:\n",
        "            best_k, best_score = k, sc\n",
        "\n",
        "    # Plot silhouette scores\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(range(min_k, max_k + 1), scores, 'bo-')\n",
        "    plt.axvline(x=best_k, color='red', linestyle='--', alpha=0.7, label=f'Best K={best_k}')\n",
        "    plt.xlabel('Number of Clusters (K)')\n",
        "    plt.ylabel('Silhouette Score')\n",
        "    plt.title('K Selection via Silhouette Analysis')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    km = KMeans(n_clusters=best_k, random_state=random_state, n_init=10).fit(X_array)\n",
        "    return km, best_k, best_score\n",
        "\n",
        "def summarize_clusters(df, labels, cols):\n",
        "    \"\"\"Generate comprehensive cluster summary statistics\"\"\"\n",
        "    dfc = df.copy()\n",
        "    dfc[\"state\"] = labels\n",
        "    stats = []\n",
        "\n",
        "    for s, grp in dfc.groupby(\"state\"):\n",
        "        row = {\"state\": int(s), \"count\": len(grp)}\n",
        "\n",
        "        # Basic statistics\n",
        "        for c in cols:\n",
        "            if c in grp.columns:\n",
        "                row[f\"{c}_mean\"] = grp[c].mean()\n",
        "                row[f\"{c}_std\"] = grp[c].std()\n",
        "                row[f\"{c}_p10\"] = grp[c].quantile(0.1)\n",
        "                row[f\"{c}_p90\"] = grp[c].quantile(0.9)\n",
        "\n",
        "        # Duration statistics\n",
        "        state_mask = (dfc[\"state\"] == s)\n",
        "        state_mask = pd.Series(state_mask.values, index=dfc.index)\n",
        "        state_events = contiguous_events(state_mask, 1, 0)\n",
        "        if state_events:\n",
        "            durations = [((e - s) / pd.Timedelta(minutes=CONFIG[\"freq_minutes\"]) + 1) * CONFIG[\"freq_minutes\"]\n",
        "                        for s, e in state_events]\n",
        "            row[\"avg_duration_min\"] = np.mean(durations)\n",
        "            row[\"event_count\"] = len(state_events)\n",
        "        else:\n",
        "            row[\"avg_duration_min\"] = 0\n",
        "            row[\"event_count\"] = 0\n",
        "\n",
        "        stats.append(row)\n",
        "\n",
        "    return pd.DataFrame(stats).sort_values(\"state\")\n",
        "\n",
        "def make_state_strip_plot(tidx, labels, path):\n",
        "    \"\"\"Create operational states timeline visualization\"\"\"\n",
        "    dfp = pd.DataFrame({\"t\": tidx, \"state\": labels}).set_index(\"t\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(15, 4))\n",
        "    scatter = ax.scatter(dfp.index, dfp[\"state\"], s=3, c=dfp[\"state\"],\n",
        "                        cmap=\"tab10\", marker=\"|\", alpha=0.8)\n",
        "    ax.set_ylabel(\"Operational State\")\n",
        "    ax.set_title(\"Machine Operational States Over Time\")\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    cbar = plt.colorbar(scatter, ax=ax)\n",
        "    cbar.set_label(\"State ID\")\n",
        "\n",
        "    savefig(path)\n",
        "    plt.show()\n",
        "\n",
        "print(\" Clustering functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_-CfnjVJ_jI"
      },
      "source": [
        "#  Clustering Execution\n",
        "Performing operational state clustering on active (non-shutdown) data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clustering operational states...\n",
            "Active data points: 292439 (77.2% of total)\n",
            "Building cluster features...\n",
            "Feature matrix shape: (289121, 54)\n",
            "Standardizing features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\16422097.py:127: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_fill = X.fillna(method='ffill').fillna(method='bfill')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting optimal number of clusters...\n",
            "Dataset size: 289121 points, 54 features\n",
            "Large dataset detected, using elbow method for speed\n",
            "Using elbow method for faster cluster selection:\n",
            "  K=3... inertia=11346467\n",
            "  K=4... inertia=10568853\n",
            "  K=5... inertia=9986639\n",
            "  K=6... inertia=9573401\n",
            "✓ Selected 5 clusters with inertia: 9986639\n",
            "\n",
            "Cluster Summary:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>state</th>\n",
              "      <th>count</th>\n",
              "      <th>avg_duration_min</th>\n",
              "      <th>event_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>76661</td>\n",
              "      <td>132.354751</td>\n",
              "      <td>2926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>73646</td>\n",
              "      <td>85.804293</td>\n",
              "      <td>4333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>66109</td>\n",
              "      <td>77.201944</td>\n",
              "      <td>4630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>62337</td>\n",
              "      <td>50.804016</td>\n",
              "      <td>6225</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>10368</td>\n",
              "      <td>1075.601415</td>\n",
              "      <td>424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   state  count  avg_duration_min  event_count\n",
              "0      0  76661        132.354751         2926\n",
              "1      1  73646         85.804293         4333\n",
              "2      2  66109         77.201944         4630\n",
              "3      3  62337         50.804016         6225\n",
              "4      4  10368       1075.601415          424"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Cluster analysis complete\n"
          ]
        }
      ],
      "source": [
        "def choose_kmeans_k_optimized(X_array, min_k, max_k, random_state, sample_size=5000):\n",
        "    \"\"\"\n",
        "    Optimized version of choose_kmeans_k that handles large datasets more efficiently\n",
        "    \"\"\"\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn.metrics import silhouette_score\n",
        "    import numpy as np\n",
        "    \n",
        "    print(f\"Original dataset size: {X_array.shape}\")\n",
        "    \n",
        "    # For large datasets, use sampling for silhouette score calculation\n",
        "    if len(X_array) > sample_size:\n",
        "        print(f\"Dataset is large, using sample of {sample_size} points for silhouette calculation\")\n",
        "        sample_indices = np.random.RandomState(random_state).choice(\n",
        "            len(X_array), size=sample_size, replace=False\n",
        "        )\n",
        "        X_sample = X_array[sample_indices]\n",
        "    else:\n",
        "        X_sample = X_array\n",
        "        sample_indices = np.arange(len(X_array))\n",
        "    \n",
        "    best_score = -1\n",
        "    best_k = min_k\n",
        "    best_km = None\n",
        "    scores = []\n",
        "    \n",
        "    print(\"Testing cluster counts:\")\n",
        "    for k in range(min_k, max_k + 1):\n",
        "        print(f\"  K={k}...\", end=\"\")\n",
        "        \n",
        "        # Fit on full dataset but evaluate on sample\n",
        "        km = KMeans(n_clusters=k, random_state=random_state, n_init=10, max_iter=300)\n",
        "        km.fit(X_array)\n",
        "        \n",
        "        # Get labels for sample\n",
        "        if len(X_array) > sample_size:\n",
        "            sample_labels = km.predict(X_sample)\n",
        "        else:\n",
        "            sample_labels = km.labels_\n",
        "        \n",
        "        # Calculate silhouette score on sample\n",
        "        sc = silhouette_score(X_sample, sample_labels)\n",
        "        scores.append(sc)\n",
        "        \n",
        "        print(f\" silhouette={sc:.3f}\")\n",
        "        \n",
        "        if sc > best_score:\n",
        "            best_score = sc\n",
        "            best_k = k\n",
        "            best_km = km\n",
        "    \n",
        "    return best_km, best_k, best_score\n",
        "\n",
        "\n",
        "def choose_kmeans_k_elbow(X_array, min_k, max_k, random_state):\n",
        "    \"\"\"\n",
        "    Alternative method using elbow method (inertia) which is much faster\n",
        "    \"\"\"\n",
        "    from sklearn.cluster import KMeans\n",
        "    import numpy as np\n",
        "    \n",
        "    inertias = []\n",
        "    k_range = range(min_k, max_k + 1)\n",
        "    \n",
        "    print(\"Using elbow method for faster cluster selection:\")\n",
        "    for k in k_range:\n",
        "        print(f\"  K={k}...\", end=\"\")\n",
        "        km = KMeans(n_clusters=k, random_state=random_state, n_init=10, max_iter=300)\n",
        "        km.fit(X_array)\n",
        "        inertias.append(km.inertia_)\n",
        "        print(f\" inertia={km.inertia_:.0f}\")\n",
        "    \n",
        "    # Simple elbow detection: find the point where the rate of decrease slows down\n",
        "    if len(inertias) >= 3:\n",
        "        # Calculate rate of change\n",
        "        rates = []\n",
        "        for i in range(1, len(inertias)):\n",
        "            rates.append(inertias[i-1] - inertias[i])\n",
        "        \n",
        "        # Find where the rate of improvement drops significantly\n",
        "        rate_changes = []\n",
        "        for i in range(1, len(rates)):\n",
        "            rate_changes.append(rates[i-1] - rates[i])\n",
        "        \n",
        "        if rate_changes:\n",
        "            # Pick the k where the rate change is maximum (elbow point)\n",
        "            elbow_idx = np.argmax(rate_changes)\n",
        "            best_k = min_k + elbow_idx + 2  # +2 because of indexing offset\n",
        "        else:\n",
        "            best_k = min_k + 1\n",
        "    else:\n",
        "        best_k = min_k + 1\n",
        "    \n",
        "    # Fit final model with best k\n",
        "    best_km = KMeans(n_clusters=best_k, random_state=random_state, n_init=10)\n",
        "    best_km.fit(X_array)\n",
        "    \n",
        "    return best_km, best_k, inertias[best_k - min_k]\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXECUTE CLUSTERING ON ACTIVE DATA\n",
        "# ============================================================================\n",
        "\n",
        "print(\"Clustering operational states...\")\n",
        "\n",
        "# Use only active (non-shutdown) data\n",
        "active = df[~shutdown_mask].copy()\n",
        "print(f\"Active data points: {len(active)} ({len(active)/len(df)*100:.1f}% of total)\")\n",
        "\n",
        "if len(active) < 100:\n",
        "    print(\"Warning: Very few active data points for clustering\")\n",
        "else:\n",
        "    # Build clustering features\n",
        "    print(\"Building cluster features...\")\n",
        "    X = build_cluster_features(active[available_vars], available_vars)\n",
        "\n",
        "    if X.empty:\n",
        "        print(\"Warning: No features available after dropna\")\n",
        "    else:\n",
        "        print(f\"Feature matrix shape: {X.shape}\")\n",
        "        active_X = active.loc[X.index].copy()\n",
        "\n",
        "        # Standardize features\n",
        "        print(\"Standardizing features...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_fill = X.fillna(method='ffill').fillna(method='bfill')\n",
        "        Xs = scaler.fit_transform(X_fill)\n",
        "\n",
        "        # Choose optimal K - OPTIMIZED VERSION\n",
        "        print(\"Selecting optimal number of clusters...\")\n",
        "        print(f\"Dataset size: {Xs.shape[0]} points, {Xs.shape[1]} features\")\n",
        "\n",
        "        # Choose method based on dataset size\n",
        "        if Xs.shape[0] > 10000:\n",
        "            print(\"Large dataset detected, using elbow method for speed\")\n",
        "            km, best_k, best_metric = choose_kmeans_k_elbow(\n",
        "                Xs,\n",
        "                CONFIG['cluster']['min_k'],\n",
        "                CONFIG['cluster']['max_k'],\n",
        "                CONFIG['random_state']\n",
        "            )\n",
        "            print(f\"✓ Selected {best_k} clusters with inertia: {best_metric:.0f}\")\n",
        "        else:\n",
        "            print(\"Using optimized silhouette method with sampling\")\n",
        "            km, best_k, best_score = choose_kmeans_k_optimized(\n",
        "                Xs,\n",
        "                CONFIG['cluster']['min_k'],\n",
        "                CONFIG['cluster']['max_k'],\n",
        "                CONFIG['random_state']\n",
        "            )\n",
        "            print(f\"✓ Selected {best_k} clusters with silhouette score: {best_score:.3f}\")\n",
        "\n",
        "        labels = km.predict(Xs)\n",
        "\n",
        "        # Generate cluster summary\n",
        "        cluster_summary = summarize_clusters(active_X.loc[X.index, available_vars], labels, available_vars)\n",
        "        cluster_summary.to_csv(os.path.join(CONFIG['outputs_dir'], 'clusters_summary.csv'), index=False)\n",
        "\n",
        "        print(\"\\nCluster Summary:\")\n",
        "        display(cluster_summary[['state', 'count', 'avg_duration_min', 'event_count']].head(10))\n",
        "\n",
        "        print(\"Cluster analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above is AI generated as the in clustering funtion below the silhouette_score function has O(n²) time complexity, making it extremely slow for large datasets . When the clustering optimization loop tries different values of k, each iteration becomes progressively slower. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "gmSZmrC3KEYP",
        "outputId": "828ed23f-5c99-4f5e-e2ce-327fa121c163"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clustering operational states...\n",
            "Active data points: 292439 (77.2% of total)\n",
            "Building cluster features...\n",
            "Feature matrix shape: (289121, 54)\n",
            "Standardizing features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\3571437509.py:24: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_fill = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selecting optimal number of clusters...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Choose optimal K\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelecting optimal number of clusters...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m km, best_k, best_score \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_kmeans_k\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mXs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcluster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcluster\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m labels \u001b[38;5;241m=\u001b[39m km\u001b[38;5;241m.\u001b[39mpredict(Xs)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Selected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_k\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m clusters with silhouette score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[22], line 10\u001b[0m, in \u001b[0;36mchoose_kmeans_k\u001b[1;34m(X_array, min_k, max_k, random_state)\u001b[0m\n\u001b[0;32m      8\u001b[0m km \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39mrandom_state, n_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m km\u001b[38;5;241m.\u001b[39mfit_predict(X_array)\n\u001b[1;32m---> 10\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43msilhouette_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(sc)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;241m>\u001b[39m best_score:\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:141\u001b[0m, in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[1;32m--> 141\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43msilhouette_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:305\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    301\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[0;32m    302\u001b[0m reduce_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[0;32m    303\u001b[0m     _silhouette_reduce, labels\u001b[38;5;241m=\u001b[39mlabels, label_freqs\u001b[38;5;241m=\u001b[39mlabel_freqs\n\u001b[0;32m    304\u001b[0m )\n\u001b[1;32m--> 305\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m intra_clust_dists, inter_clust_dists \u001b[38;5;241m=\u001b[39m results\n\u001b[0;32m    307\u001b[0m intra_clust_dists \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(intra_clust_dists)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:2172\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[1;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[0;32m   2170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2171\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[1;32m-> 2172\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   2174\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2175\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[0;32m   2176\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[0;32m   2177\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[0;32m   2178\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:2375\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[1;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[0;32m   2372\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m   2373\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m-> 2375\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:1893\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[1;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[0;32m   1890\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 1893\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1895\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[0;32m   1896\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    188\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:372\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m    367\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    368\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    370\u001b[0m         )\n\u001b[1;32m--> 372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\metrics\\pairwise.py:410\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[1;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[0;32m    408\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX\n\u001b[0;32m    409\u001b[0m     distances \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY\n\u001b[1;32m--> 410\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdistances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;66;03m# Ensure that distances between vectors and themselves are set to 0.0.\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;66;03m# This may not be the case due to floating point rounding errors.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Execute clustering on active data\n",
        "print(\"Clustering operational states...\")\n",
        "\n",
        "# Use only active (non-shutdown) data\n",
        "active = df[~shutdown_mask].copy()\n",
        "print(f\"Active data points: {len(active)} ({len(active)/len(df)*100:.1f}% of total)\")\n",
        "\n",
        "if len(active) < 100:\n",
        "    print(\"Warning: Very few active data points for clustering\")\n",
        "else:\n",
        "    # Build clustering features\n",
        "    print(\"Building cluster features...\")\n",
        "    X = build_cluster_features(active[available_vars], available_vars)\n",
        "\n",
        "    if X.empty:\n",
        "        print(\"Warning: No features available after dropna\")\n",
        "    else:\n",
        "        print(f\"Feature matrix shape: {X.shape}\")\n",
        "        active_X = active.loc[X.index].copy()\n",
        "\n",
        "        # Standardize features\n",
        "        print(\"Standardizing features...\")\n",
        "        scaler = StandardScaler()\n",
        "        X_fill = X.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "        Xs = scaler.fit_transform(X_fill)\n",
        "\n",
        "        # Choose optimal K\n",
        "        print(\"Selecting optimal number of clusters...\")\n",
        "        km, best_k, best_score = choose_kmeans_k(\n",
        "            Xs,\n",
        "            CONFIG[\"cluster\"][\"min_k\"],\n",
        "            CONFIG[\"cluster\"][\"max_k\"],\n",
        "            CONFIG[\"random_state\"]\n",
        "        )\n",
        "\n",
        "        labels = km.predict(Xs)\n",
        "        print(f\"✓ Selected {best_k} clusters with silhouette score: {best_score:.3f}\")\n",
        "\n",
        "        # Generate cluster summary\n",
        "        cluster_summary = summarize_clusters(active_X.loc[X.index, available_vars], labels, available_vars)\n",
        "        cluster_summary.to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"clusters_summary.csv\"), index=False)\n",
        "\n",
        "        print(\"\\nCluster Summary:\")\n",
        "        display(cluster_summary[['state', 'count', 'avg_duration_min', 'event_count']].head(10))\n",
        "\n",
        "        print(\"✓ Cluster analysis complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxN4MYPpORVe"
      },
      "source": [
        "#  Clustering Visualizations\n",
        "Creating state timeline plots, distribution charts, and transition matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "WAPXVT30OQ9X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating clustering visualizations...\n",
            " State visualizations complete\n"
          ]
        }
      ],
      "source": [
        "# Clustering visualizations\n",
        "if 'labels' in locals():\n",
        "    print(\"Creating clustering visualizations...\")\n",
        "\n",
        "    # State timeline\n",
        "    make_state_strip_plot(X.index, labels,\n",
        "                         os.path.join(CONFIG[\"plots_dir\"], \"operational_states_timeline.png\"))\n",
        "\n",
        "    # State distribution\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    state_counts = pd.Series(labels).value_counts().sort_index()\n",
        "    colors = plt.cm.tab10(np.arange(len(state_counts)))\n",
        "\n",
        "    plt.pie(state_counts.values, labels=[f'State {i}' for i in state_counts.index],\n",
        "            autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "    plt.title(\"Distribution of Operational States\")\n",
        "    savefig(os.path.join(CONFIG[\"plots_dir\"], \"state_distribution.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # State transition analysis\n",
        "    state_series = pd.Series(labels, index=X.index)\n",
        "    transitions = pd.DataFrame({\n",
        "        'from_state': state_series.shift(1),\n",
        "        'to_state': state_series\n",
        "    }).dropna()\n",
        "\n",
        "    transition_matrix = pd.crosstab(transitions['from_state'], transitions['to_state'],\n",
        "                                  normalize='index') * 100\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(transition_matrix, annot=True, fmt='.1f', cmap='Blues',\n",
        "                cbar_kws={'label': 'Transition Probability (%)'})\n",
        "    plt.title('State Transition Matrix (%)')\n",
        "    plt.xlabel('To State')\n",
        "    plt.ylabel('From State')\n",
        "    savefig(os.path.join(CONFIG[\"plots_dir\"], \"state_transitions.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    print(\" State visualizations complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQmMohqMO2ow"
      },
      "source": [
        "#  Anomaly Detection Functions\n",
        "Defining Isolation Forest-based anomaly detection with configurable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iR3KdGioOzho",
        "outputId": "a97e3c82-c7da-4909-aed8-e95f173f98cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Anomaly detection functions defined\n"
          ]
        }
      ],
      "source": [
        "# Anomaly detection functions\n",
        "def anomalies_by_state(df_active, labels, cols, contamination, merge_gap_steps, min_event_steps):\n",
        "    \"\"\"State-aware anomaly detection using Isolation Forest\"\"\"\n",
        "    out_rows = []\n",
        "    dfc = df_active.copy()\n",
        "    dfc[\"state\"] = labels\n",
        "\n",
        "    print(\"Detecting anomalies per state:\")\n",
        "    for s, grp in dfc.groupby(\"state\"):\n",
        "        print(f\"  State {s}: {len(grp)} samples\", end=\"\")\n",
        "\n",
        "        feats = grp[cols].copy()\n",
        "        feats = feats.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "\n",
        "        if len(feats) < 50:\n",
        "            print(\" (skipped - insufficient data)\")\n",
        "            continue\n",
        "\n",
        "        # Fit Isolation Forest\n",
        "        scaler = StandardScaler()\n",
        "        Z = scaler.fit_transform(feats)\n",
        "\n",
        "        iso = IsolationForest(\n",
        "            n_estimators=200,\n",
        "            contamination=contamination,\n",
        "            random_state=CONFIG[\"random_state\"],\n",
        "            n_jobs=-1\n",
        "        ).fit(Z)\n",
        "\n",
        "        scores = -iso.decision_function(Z)\n",
        "        grp_idx = grp.index\n",
        "        mask = pd.Series(scores > np.quantile(scores, 1 - contamination), index=grp_idx)\n",
        "\n",
        "        # Group into events\n",
        "        events = contiguous_events(mask, min_event_steps, merge_gap_steps)\n",
        "        print(f\" -> {len(events)} anomaly events\")\n",
        "\n",
        "        for s_ts, e_ts in events:\n",
        "            sub = feats.loc[s_ts:e_ts]\n",
        "            z_scores = (sub - feats.mean()) / (feats.std() + 1e-6)\n",
        "            implicated_vars = z_scores.abs().mean().sort_values(ascending=False).head(3).index.tolist()\n",
        "\n",
        "            out_rows.append({\n",
        "                \"start\": s_ts,\n",
        "                \"end\": e_ts,\n",
        "                \"duration_min\": int(((e_ts - s_ts) / pd.Timedelta(minutes=CONFIG[\"freq_minutes\"])) + 1) * CONFIG[\"freq_minutes\"],\n",
        "                \"state\": int(s),\n",
        "                \"top_variables\": \",\".join(implicated_vars),\n",
        "                \"severity\": float(z_scores.abs().mean().mean())\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(out_rows).sort_values(\"start\") if out_rows else pd.DataFrame()\n",
        "\n",
        "print(\"Anomaly detection functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sioHm0gMO6Qp"
      },
      "source": [
        "# Anomaly Detection Execution\n",
        "Detecting anomalous events across operational states and ranking by severity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xn2ixF4PMxsU",
        "outputId": "c7d13fcf-ed4e-4d8b-b39e-fbebb7de83f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detecting anomalous events...\n",
            "Detecting anomalies per state:\n",
            "  State 0: 76661 samples"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\3014566497.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  feats = feats.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -> 253 anomaly events\n",
            "  State 1: 73646 samples"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\3014566497.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  feats = feats.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -> 197 anomaly events\n",
            "  State 2: 66109 samples"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\3014566497.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  feats = feats.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -> 208 anomaly events\n",
            "  State 3: 62337 samples"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\3014566497.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  feats = feats.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -> 203 anomaly events\n",
            "  State 4: 10368 samples"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\3014566497.py:13: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  feats = feats.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " -> 43 anomaly events\n",
            "\n",
            "✓ Found 904 anomalous events\n",
            "Average anomaly duration: 117.0 minutes\n",
            "Anomalies by state:\n",
            "  State 0: 253 events\n",
            "  State 1: 197 events\n",
            "  State 2: 208 events\n",
            "  State 3: 203 events\n",
            "  State 4: 43 events\n",
            "\n",
            "Top 5 most severe anomalies:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>duration_min</th>\n",
              "      <th>state</th>\n",
              "      <th>severity</th>\n",
              "      <th>top_variables</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>2020-07-18 13:00:00</td>\n",
              "      <td>2020-07-18 13:05:00</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4.395200</td>\n",
              "      <td>Cyclone_Inlet_Draft,Cyclone_Outlet_Gas_draft,C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>2018-08-01 00:00:00</td>\n",
              "      <td>2018-08-01 00:15:00</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>4.343986</td>\n",
              "      <td>Cyclone_Outlet_Gas_draft,Cyclone_Inlet_Draft,C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>2020-07-19 04:45:00</td>\n",
              "      <td>2020-07-19 04:50:00</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>4.126002</td>\n",
              "      <td>Cyclone_Inlet_Draft,Cyclone_Outlet_Gas_draft,C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>2017-12-12 11:00:00</td>\n",
              "      <td>2017-12-12 11:05:00</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>3.853583</td>\n",
              "      <td>Cyclone_Inlet_Gas_Temp,Cyclone_Inlet_Draft,Cyc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>2018-06-14 15:20:00</td>\n",
              "      <td>2018-06-14 15:25:00</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>3.850898</td>\n",
              "      <td>Cyclone_Outlet_Gas_draft,Cyclone_Inlet_Draft,C...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  start                 end  duration_min  state  severity  \\\n",
              "245 2020-07-18 13:00:00 2020-07-18 13:05:00            10      0  4.395200   \n",
              "120 2018-08-01 00:00:00 2018-08-01 00:15:00            20      0  4.343986   \n",
              "246 2020-07-19 04:45:00 2020-07-19 04:50:00            10      0  4.126002   \n",
              "330 2017-12-12 11:00:00 2017-12-12 11:05:00            10      1  3.853583   \n",
              "51  2018-06-14 15:20:00 2018-06-14 15:25:00            10      0  3.850898   \n",
              "\n",
              "                                         top_variables  \n",
              "245  Cyclone_Inlet_Draft,Cyclone_Outlet_Gas_draft,C...  \n",
              "120  Cyclone_Outlet_Gas_draft,Cyclone_Inlet_Draft,C...  \n",
              "246  Cyclone_Inlet_Draft,Cyclone_Outlet_Gas_draft,C...  \n",
              "330  Cyclone_Inlet_Gas_Temp,Cyclone_Inlet_Draft,Cyc...  \n",
              "51   Cyclone_Outlet_Gas_draft,Cyclone_Inlet_Draft,C...  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Execute anomaly detection\n",
        "if 'labels' in locals() and 'active_X' in locals():\n",
        "    print(\"Detecting anomalous events...\")\n",
        "\n",
        "    contam = CONFIG[\"anomaly\"][\"contamination\"]\n",
        "    merge_gap_steps = int(CONFIG[\"anomaly\"][\"merge_gap_minutes\"] / CONFIG[\"freq_minutes\"])\n",
        "    min_event_steps = int(CONFIG[\"anomaly\"][\"min_event_minutes\"] / CONFIG[\"freq_minutes\"])\n",
        "\n",
        "    anomalies = anomalies_by_state(\n",
        "        active_X.loc[X.index, available_vars],\n",
        "        labels,\n",
        "        available_vars,\n",
        "        contam,\n",
        "        merge_gap_steps,\n",
        "        min_event_steps\n",
        "    )\n",
        "\n",
        "    if not anomalies.empty:\n",
        "        anomalies.to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"anomalous_periods.csv\"), index=False)\n",
        "\n",
        "        print(f\"\\n✓ Found {len(anomalies)} anomalous events\")\n",
        "        print(f\"Average anomaly duration: {anomalies['duration_min'].mean():.1f} minutes\")\n",
        "\n",
        "        # Anomalies by state\n",
        "        if 'state' in anomalies.columns:\n",
        "            state_anomaly_counts = anomalies['state'].value_counts().sort_index()\n",
        "            print(f\"Anomalies by state:\")\n",
        "            for state, count in state_anomaly_counts.items():\n",
        "                print(f\"  State {state}: {count} events\")\n",
        "\n",
        "        print(\"\\nTop 5 most severe anomalies:\")\n",
        "        top_anomalies = anomalies.nlargest(5, 'severity')\n",
        "        display(top_anomalies[['start', 'end', 'duration_min', 'state', 'severity', 'top_variables']])\n",
        "\n",
        "    else:\n",
        "        print(\"No anomalies detected\")\n",
        "        pd.DataFrame().to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"anomalous_periods.csv\"), index=False)\n",
        "\n",
        "else:\n",
        "    print(\"Skipping anomaly detection - clustering not completed\")\n",
        "    pd.DataFrame().to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"anomalous_periods.csv\"), index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FuNX-ykSOEd"
      },
      "source": [
        "#  Anomaly Visualizations\n",
        "Visualizing anomaly timeline, severity distribution, and implicated variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hczyXFr1O-ad",
        "outputId": "75878731-e267-4b38-a542-a47cc4bfb1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating anomaly visualizations...\n",
            "✓ Anomaly visualizations complete\n"
          ]
        }
      ],
      "source": [
        "# Anomaly visualizations\n",
        "if 'anomalies' in locals() and not anomalies.empty:\n",
        "    print(\"Creating anomaly visualizations...\")\n",
        "\n",
        "    # Select top anomalies for detailed plots\n",
        "    top_anomalies = anomalies.nlargest(min(5, len(anomalies)), 'severity')\n",
        "\n",
        "    for i, (idx, row) in enumerate(top_anomalies.iterrows()):\n",
        "        s, e = row[\"start\"], row[\"end\"]\n",
        "        state = row[\"state\"]\n",
        "\n",
        "        # Context window (2 hours before/after)\n",
        "        context_start = s - pd.Timedelta(hours=2)\n",
        "        context_end = e + pd.Timedelta(hours=2)\n",
        "        rng = slice(context_start, context_end)\n",
        "\n",
        "        fig, axes = plt.subplots(len(available_vars), 1,\n",
        "                               figsize=(15, 2.5*len(available_vars)))\n",
        "        if len(available_vars) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for j, v in enumerate(available_vars):\n",
        "            data_slice = df.loc[rng, v].dropna()\n",
        "            if not data_slice.empty:\n",
        "                # Z-score normalization\n",
        "                normalized = (data_slice - df[v].mean()) / df[v].std()\n",
        "                axes[j].plot(data_slice.index, normalized, label=v, lw=0.8, alpha=0.8)\n",
        "\n",
        "                # Highlight anomaly period\n",
        "                axes[j].axvspan(s, e, color=\"red\", alpha=0.3,\n",
        "                               label=\"Anomaly Period\" if j == 0 else \"\")\n",
        "\n",
        "                # Reference lines\n",
        "                axes[j].axhline(y=2, color='orange', linestyle='--', alpha=0.5)\n",
        "                axes[j].axhline(y=-2, color='orange', linestyle='--', alpha=0.5)\n",
        "\n",
        "                axes[j].set_ylabel(f\"{v}\\n(z-score)\")\n",
        "                axes[j].grid(True, alpha=0.3)\n",
        "                axes[j].legend()\n",
        "\n",
        "        plt.suptitle(f\"Anomaly Event {i+1} (State {state})\\n\"\n",
        "                    f\"{s.strftime('%Y-%m-%d %H:%M')} to {e.strftime('%Y-%m-%d %H:%M')}\\n\"\n",
        "                    f\"Severity: {row['severity']:.3f}, Variables: {row['top_variables']}\",\n",
        "                    fontsize=12)\n",
        "\n",
        "        savefig(os.path.join(CONFIG[\"plots_dir\"], f\"anomaly_event_{i+1}.png\"))\n",
        "        plt.show()\n",
        "\n",
        "    # Anomaly frequency over time\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    anomaly_dates = pd.to_datetime(anomalies['start']).dt.date\n",
        "    anomaly_counts = pd.Series(anomaly_dates).value_counts().sort_index()\n",
        "\n",
        "    plt.plot(anomaly_counts.index, anomaly_counts.values, 'ro-', alpha=0.7)\n",
        "    plt.title('Anomaly Events Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Number of Events')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    savefig(os.path.join(CONFIG[\"plots_dir\"], \"anomaly_frequency.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✓ Anomaly visualizations complete\")\n",
        "else:\n",
        "    print(\"No anomalies to visualize\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wmPZ5p2SWg2"
      },
      "source": [
        "#  Forecasting Functions\n",
        "Defining Ridge regression-based time series forecasting pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Forecasting functions defined\n"
          ]
        }
      ],
      "source": [
        "# Forecasting functions\n",
        "def forecasting_pipeline(df, target, horizon, model_type):\n",
        "    \"\"\"Complete forecasting pipeline with model comparison\"\"\"\n",
        "\n",
        "    print(f\"Building forecasting model for {target}...\")\n",
        "    y = df[target].astype(float)\n",
        "\n",
        "    # Create feature matrix\n",
        "    X = pd.DataFrame(index=y.index)\n",
        "    for lag in [1, 2, 3, 6, 12, 24]:\n",
        "        X[f\"lag_{lag}\"] = y.shift(lag)\n",
        "\n",
        "    # Rolling features\n",
        "    X[\"rollmean_12\"] = y.rolling(12, min_periods=6).mean()\n",
        "    X[\"rollstd_12\"] = y.rolling(12, min_periods=6).std()\n",
        "\n",
        "    # Combine and clean\n",
        "    data = pd.concat([y, X], axis=1).dropna()\n",
        "    print(f\"Feature matrix shape: {data.shape}\")\n",
        "\n",
        "    y_clean = data[target]\n",
        "    X_clean = data.drop(columns=[target])\n",
        "\n",
        "    # Train/test split\n",
        "    split_idx = int(len(data) * CONFIG[\"forecast\"][\"train_ratio\"])\n",
        "    X_train, X_test = X_clean.iloc[:split_idx], X_clean.iloc[split_idx:]\n",
        "    y_train, y_test = y_clean.iloc[:split_idx], y_clean.iloc[split_idx:]\n",
        "\n",
        "    print(f\"Train: {len(y_train)}, Test: {len(y_test)} samples\")\n",
        "\n",
        "    # Baseline: Persistence\n",
        "    y_pred_baseline = y_test.shift(1).fillna(y_test.iloc[0])\n",
        "\n",
        "    # Advanced model - Ridge regression only\n",
        "    print(\"Fitting Ridge regression...\")\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train.fillna(method='ffill'))\n",
        "    X_test_scaled = scaler.transform(X_test.fillna(method='ffill'))\n",
        "\n",
        "    ridge = Ridge(alpha=1.0, random_state=CONFIG[\"random_state\"])\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred_model = pd.Series(ridge.predict(X_test_scaled), index=y_test.index)\n",
        "    print(\"✓ Ridge fitted successfully\")\n",
        "\n",
        "    # Calculate metrics\n",
        "    def rmse(actual, predicted):\n",
        "        return float(np.sqrt(np.mean((np.array(actual) - np.array(predicted))**2)))\n",
        "\n",
        "    def mae(actual, predicted):\n",
        "        return float(np.mean(np.abs(np.array(actual) - np.array(predicted))))\n",
        "\n",
        "    metrics = {\n",
        "        \"model_type\": \"ridge\",\n",
        "        \"rmse_model\": rmse(y_test, y_pred_model),\n",
        "        \"mae_model\": mae(y_test, y_pred_model),\n",
        "        \"rmse_baseline\": rmse(y_test, y_pred_baseline),\n",
        "        \"mae_baseline\": mae(y_test, y_pred_baseline),\n",
        "        \"improvement_rmse\": ((rmse(y_test, y_pred_baseline) - rmse(y_test, y_pred_model)) / rmse(y_test, y_pred_baseline)) * 100,\n",
        "        \"improvement_mae\": ((mae(y_test, y_pred_baseline) - mae(y_test, y_pred_model)) / mae(y_test, y_pred_baseline)) * 100\n",
        "    }\n",
        "\n",
        "    print(f\"\\nForecasting Results:\")\n",
        "    print(f\"Model RMSE: {metrics['rmse_model']:.3f}\")\n",
        "    print(f\"Baseline RMSE: {metrics['rmse_baseline']:.3f}\")\n",
        "    print(f\"RMSE Improvement: {metrics['improvement_rmse']:.1f}%\")\n",
        "\n",
        "    # Save results\n",
        "    with open(os.path.join(CONFIG[\"outputs_dir\"], \"forecast_metrics.json\"), \"w\") as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "\n",
        "    forecasts_df = pd.DataFrame({\n",
        "        \"timestamp\": y_test.index,\n",
        "        \"y_true\": y_test.values,\n",
        "        \"y_pred_baseline\": y_pred_baseline.values,\n",
        "        \"y_pred_model\": y_pred_model.values,\n",
        "    })\n",
        "    forecasts_df.to_csv(os.path.join(CONFIG[\"outputs_dir\"], \"forecasts.csv\"), index=False)\n",
        "\n",
        "    return y_test, y_pred_baseline, y_pred_model, metrics\n",
        "\n",
        "\n",
        "print(\" Forecasting functions defined\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx9weMJ8SfJM"
      },
      "source": [
        "Forecasting Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "dv-M7_ecScE_",
        "outputId": "23235c05-a3e1-4a25-f48f-702f9e0f7433"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running forecasting...\n",
            "Forecasting target: Cyclone_Inlet_Gas_Temp\n",
            "Masking shutdown periods...\n",
            "Masked 86141 shutdown points\n",
            "Data for forecasting: 378580 points\n",
            "Missing values: 0\n",
            "Building forecasting model for Cyclone_Inlet_Gas_Temp...\n",
            "Feature matrix shape: (378556, 9)\n",
            "Train: 302844, Test: 75712 samples\n",
            "Fitting Ridge regression...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\2263806623.py:37: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_train_scaled = scaler.fit_transform(X_train.fillna(method='ffill'))\n",
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_24972\\2263806623.py:38: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X_test_scaled = scaler.transform(X_test.fillna(method='ffill'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Ridge fitted successfully\n",
            "\n",
            "Forecasting Results:\n",
            "Model RMSE: 12.267\n",
            "Baseline RMSE: 16.396\n",
            "RMSE Improvement: 25.2%\n",
            "✓ Forecasting complete\n"
          ]
        }
      ],
      "source": [
        "# Execute forecasting\n",
        "print(\"Running forecasting...\")\n",
        "\n",
        "# Select target variable\n",
        "target_var = CONFIG[\"forecast\"][\"target\"] if CONFIG[\"forecast\"][\"target\"] in available_vars else available_vars[0]\n",
        "print(f\"Forecasting target: {target_var}\")\n",
        "\n",
        "# Prepare clean data (mask shutdowns)\n",
        "clean_for_forecast = df[[target_var]].copy()\n",
        "\n",
        "if 'shutdown_mask' in locals():\n",
        "    print(\"Masking shutdown periods...\")\n",
        "    clean_for_forecast.loc[shutdown_mask, target_var] = np.nan\n",
        "    print(f\"Masked {shutdown_mask.sum()} shutdown points\")\n",
        "\n",
        "# Interpolate gaps\n",
        "clean_for_forecast[target_var] = clean_for_forecast[target_var].interpolate(limit_direction=\"both\")\n",
        "\n",
        "print(f\"Data for forecasting: {len(clean_for_forecast)} points\")\n",
        "print(f\"Missing values: {clean_for_forecast[target_var].isnull().sum()}\")\n",
        "\n",
        "# Run forecasting\n",
        "y_true, y_baseline, y_model, forecast_metrics = forecasting_pipeline(\n",
        "    clean_for_forecast,\n",
        "    target_var,\n",
        "    CONFIG[\"forecast\"][\"horizon_steps\"],\n",
        "    CONFIG[\"forecast\"][\"model\"]\n",
        ")\n",
        "\n",
        "print(\"✓ Forecasting complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOYCPW2YSoHc"
      },
      "source": [
        "Forecasting Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "muYMdGV5SrRT"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating forecasting visualizations...\n",
            " Forecasting visualizations complete\n"
          ]
        }
      ],
      "source": [
        "# Forecasting visualizations\n",
        "if 'y_true' in locals():\n",
        "    print(\"Creating forecasting visualizations...\")\n",
        "\n",
        "    # Main comparison plot\n",
        "    sample_size = min(1000, len(y_true))\n",
        "    sample_slice = slice(0, sample_size)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    plt.plot(y_true.iloc[sample_slice].index, y_true.iloc[sample_slice].values,\n",
        "             label=\"True Values\", lw=1.5, alpha=0.8, color='black')\n",
        "\n",
        "    plt.plot(y_baseline.iloc[sample_slice].index, y_baseline.iloc[sample_slice].values,\n",
        "             label=f\"Baseline (RMSE: {forecast_metrics['rmse_baseline']:.3f})\",\n",
        "             alpha=0.7, linestyle='--', color='orange')\n",
        "\n",
        "    plt.plot(y_model.iloc[sample_slice].index, y_model.iloc[sample_slice].values,\n",
        "             label=f\"Model (RMSE: {forecast_metrics['rmse_model']:.3f})\",\n",
        "             alpha=0.8, color='red')\n",
        "\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.title(f\"Forecasting Comparison - {target_var}\\n\"\n",
        "              f\"RMSE Improvement: {forecast_metrics['improvement_rmse']:.1f}%, \"\n",
        "              f\"MAE Improvement: {forecast_metrics['improvement_mae']:.1f}%\",\n",
        "              fontsize=14)\n",
        "    plt.ylabel(target_var)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    savefig(os.path.join(CONFIG[\"plots_dir\"], \"forecast_comparison.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    # Error analysis\n",
        "    baseline_errors = y_true - y_baseline\n",
        "    model_errors = y_true - y_model\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Error distributions\n",
        "    axes[0].hist(baseline_errors.dropna(), bins=50, alpha=0.7, label='Baseline', density=True)\n",
        "    axes[0].hist(model_errors.dropna(), bins=50, alpha=0.7, label='Model', density=True)\n",
        "    axes[0].set_xlabel('Prediction Error')\n",
        "    axes[0].set_ylabel('Density')\n",
        "    axes[0].set_title('Error Distribution')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Errors over time\n",
        "    sample_errors = slice(0, min(500, len(model_errors)))\n",
        "    axes[1].plot(model_errors.iloc[sample_errors].index,\n",
        "                model_errors.iloc[sample_errors].values,\n",
        "                alpha=0.8, color='red', lw=0.8)\n",
        "    axes[1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
        "    axes[1].set_xlabel('Time')\n",
        "    axes[1].set_ylabel('Model Error')\n",
        "    axes[1].set_title('Model Errors Over Time')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    savefig(os.path.join(CONFIG[\"plots_dir\"], \"forecast_error_analysis.png\"))\n",
        "    plt.show()\n",
        "\n",
        "    print(\" Forecasting visualizations complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zs_baCMSx2L"
      },
      "source": [
        "Insights Generation and Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "BYWIpKbqS0Vz",
        "outputId": "91f92e96-0bfd-4707-9ebd-99bcf81f949d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating insights and recommendations...\n",
            "============================================================\n",
            "TASK 1 ANALYSIS COMPLETE\n",
            "============================================================\n",
            "\n",
            "Key Insights:\n",
            "1. Processed 378,580 data points over 1314 days\n",
            "2. Machine availability: 77.2% (268 shutdown events)\n",
            "3. Total downtime: 7178.4 hours, avg shutdown: 1607 min\n",
            "4. Identified 5 operational states, State 0 dominant (26.5%)\n",
            "5. Detected 904 anomalous events (rate: 0.69 events/day)\n",
            "\n",
            "[DONE] All results saved to: Task1/outputs\n",
            "[DONE] All plots saved to: Task1/plots\n",
            "\n",
            "Required deliverables:\n",
            "[DONE] shutdown_periods.csv (12530 bytes)\n",
            "[DONE] anomalous_periods.csv (120720 bytes)\n",
            "[DONE] clusters_summary.csv (2466 bytes)\n",
            "[DONE] forecasts.csv (4427799 bytes)\n"
          ]
        }
      ],
      "source": [
        "# Generate insights and final summary\n",
        "print(\"Generating insights and recommendations...\")\n",
        "\n",
        "insights = []\n",
        "recommendations = []\n",
        "\n",
        "# Data quality insights\n",
        "total_points = len(df)\n",
        "missing_before = df_raw[available_vars].isnull().sum().sum()\n",
        "insights.append(f\"Processed {total_points:,} data points over {(df.index.max() - df.index.min()).days} days\")\n",
        "if missing_before > 0:\n",
        "    insights.append(f\"Handled {missing_before:,} missing values using forward-fill and interpolation\")\n",
        "\n",
        "# Shutdown insights\n",
        "if 'shut_df' in locals() and not shut_df.empty:\n",
        "    total_downtime_hours = shut_df['duration_min'].sum() / 60\n",
        "    availability = ((len(df) - shutdown_mask.sum()) / len(df)) * 100\n",
        "\n",
        "    insights.append(f\"Machine availability: {availability:.1f}% ({len(shut_df)} shutdown events)\")\n",
        "    insights.append(f\"Total downtime: {total_downtime_hours:.1f} hours, avg shutdown: {shut_df['duration_min'].mean():.0f} min\")\n",
        "\n",
        "    recommendations.extend([\n",
        "        \"Implement predictive maintenance to reduce unplanned shutdowns\",\n",
        "        \"Set up automated alerts for shutdowns exceeding 60 minutes\",\n",
        "        \"Analyze shutdown patterns for seasonal maintenance planning\"\n",
        "    ])\n",
        "\n",
        "# Clustering insights\n",
        "if 'cluster_summary' in locals() and not cluster_summary.empty:\n",
        "    n_states = len(cluster_summary)\n",
        "    dominant_state = cluster_summary.loc[cluster_summary['count'].idxmax(), 'state']\n",
        "    dominant_pct = (cluster_summary.loc[cluster_summary['count'].idxmax(), 'count'] /\n",
        "                   cluster_summary['count'].sum()) * 100\n",
        "\n",
        "    insights.append(f\"Identified {n_states} operational states, State {dominant_state} dominant ({dominant_pct:.1f}%)\")\n",
        "\n",
        "    recommendations.extend([\n",
        "        \"Monitor state transitions for early degradation detection\",\n",
        "        \"Optimize operations to maximize time in efficient states\",\n",
        "        \"Investigate short-duration states for stability improvements\"\n",
        "    ])\n",
        "\n",
        "# Anomaly insights\n",
        "if 'anomalies' in locals() and not anomalies.empty:\n",
        "    anomaly_rate = len(anomalies) / (len(df) / (24 * 60 / CONFIG[\"freq_minutes\"]))\n",
        "\n",
        "    insights.append(f\"Detected {len(anomalies)} anomalous events (rate: {anomaly_rate:.2f} events/day)\")\n",
        "\n",
        "    if 'state' in anomalies.columns:\n",
        "        most_anomalous_state = anomalies['state'].mode().iloc[0]\n",
        "        state_count = (anomalies['state'] == most_anomalous_state).sum()\n",
        "        insights.append(f\"State {most_anomalous_state} shows highest anomaly frequency ({state_count} events)\")\n",
        "\n",
        "    recommendations.extend([\n",
        "        \"Implement real-time anomaly monitoring with severity thresholds\",\n",
        "        \"Focus maintenance on high-anomaly operational states\",\n",
        "        \"Investigate top implicated variables for root cause analysis\"\n",
        "    ])\n",
        "\n",
        "# Forecasting insights\n",
        "if 'forecast_metrics' in locals():\n",
        "    improvement = forecast_metrics.get('improvement_rmse', 0)\n",
        "    if improvement > 10:\n",
        "        insights.append(f\"Forecasting model shows good performance ({improvement:.1f}% RMSE improvement)\")\n",
        "    elif improvement > 0:\n",
        "        insights.append(f\"Forecasting shows modest improvement ({improvement:.1f}% RMSE reduction)\")\n",
        "    else:\n",
        "        insights.append(\"Forecasting challenging due to high variability\")\n",
        "\n",
        "    recommendations.extend([\n",
        "        \"Use 1-hour forecasts for operational planning\",\n",
        "        \"Consider state-aware forecasting for better accuracy\",\n",
        "        \"Implement forecasting in maintenance scheduling\"\n",
        "    ])\n",
        "\n",
        "# Save comprehensive insights - FIXED: Added encoding='utf-8' to handle special characters\n",
        "insight_text = f\"\"\"\n",
        "CYCLONE MACHINE DATA ANALYSIS - COMPREHENSIVE INSIGHTS\n",
        "{'='*70}\n",
        "\n",
        "SUMMARY INSIGHTS:\n",
        "{chr(10).join([f\"{i+1}. {insight}\" for i, insight in enumerate(insights)])}\n",
        "\n",
        "ACTIONABLE RECOMMENDATIONS:\n",
        "{chr(10).join([f\"- {rec}\" for rec in recommendations])}\n",
        "\n",
        "TECHNICAL SUMMARY:\n",
        "- Dataset: {(df.index.max() - df.index.min()).days} days, {len(df):,} points at 5-min intervals\n",
        "- Variables: {len(available_vars)} sensor measurements\n",
        "- Machine availability: {((len(df) - shutdown_mask.sum()) / len(df) * 100):.1f}%\n",
        "- Operational states: {best_k if 'best_k' in locals() else 'N/A'} distinct patterns\n",
        "- Anomaly detection: {len(anomalies) if 'anomalies' in locals() and not anomalies.empty else 0} events\n",
        "- Forecasting improvement: {forecast_metrics.get('improvement_rmse', 0):.1f}% RMSE reduction\n",
        "\n",
        "DELIVERABLES GENERATED:\n",
        "[DONE] shutdown_periods.csv - {len(shut_df) if not shut_df.empty else 0} shutdown events\n",
        "[DONE] clusters_summary.csv - {best_k if 'best_k' in locals() else 0} operational state profiles\n",
        "[DONE] anomalous_periods.csv - {len(anomalies) if 'anomalies' in locals() and not anomalies.empty else 0} anomaly events\n",
        "[DONE] forecasts.csv - Model vs baseline predictions\n",
        "[DONE] Comprehensive visualizations and analysis plots\n",
        "\n",
        "NEXT STEPS:\n",
        "1. Validate findings with operations team\n",
        "2. Implement real-time monitoring dashboard\n",
        "3. Set up automated alerting system\n",
        "4. Plan predictive maintenance program\n",
        "5. Consider additional sensor integration\n",
        "\"\"\"\n",
        "\n",
        "# FIXED: Added encoding='utf-8' to handle Unicode characters\n",
        "with open(os.path.join(CONFIG[\"outputs_dir\"], \"insights_summary.txt\"), \"w\", encoding='utf-8') as f:\n",
        "    f.write(insight_text)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TASK 1 ANALYSIS COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\nKey Insights:\")\n",
        "for i, insight in enumerate(insights[:5], 1):\n",
        "    print(f\"{i}. {insight}\")\n",
        "\n",
        "print(f\"\\n[DONE] All results saved to: {CONFIG['outputs_dir']}\")\n",
        "print(f\"[DONE] All plots saved to: {CONFIG['plots_dir']}\")\n",
        "\n",
        "# Verify deliverables\n",
        "required_files = [\"shutdown_periods.csv\", \"anomalous_periods.csv\", \"clusters_summary.csv\", \"forecasts.csv\"]\n",
        "print(f\"\\nRequired deliverables:\")\n",
        "for file in required_files:\n",
        "    path = os.path.join(CONFIG[\"outputs_dir\"], file)\n",
        "    status = \"[DONE]\" if os.path.exists(path) else \"[MISSING]\"\n",
        "    size = f\"({os.path.getsize(path)} bytes)\" if os.path.exists(path) else \"\"\n",
        "    print(f\"{status} {file} {size}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
